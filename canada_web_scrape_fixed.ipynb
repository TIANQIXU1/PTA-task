{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved rules: /Users/tianqixu/Downloads/PTA-task/Canada-Chile/roo.csv  (1182 rows)\n"
     ]
    }
   ],
   "source": [
    "# canada–jordan Annex 301 scraper (fixed)\n",
    "# pip install requests beautifulsoup4 lxml pandas\n",
    "\n",
    "import re\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4.element import Tag, NavigableString\n",
    "from pathlib import Path\n",
    "\n",
    "URL = \"https://www.international.gc.ca/trade-commerce/trade-agreements-accords-commerciaux/agr-acc/chile-chili/fta-ale/annex-d-annexe.aspx?lang=eng\"\n",
    "\n",
    "OUT_RULES_CSV = \"/Users/tianqixu/Downloads/PTA-task/Canada-Chile/roo.csv\"\n",
    "OUT_NOTES_CSV = \"/Users/tianqixu/Downloads/PTA-task/Canada-Chile/annex301_notes.csv\"\n",
    "Path(OUT_RULES_CSV).parent.mkdir(parents=True, exist_ok=True)\n",
    "Path(OUT_NOTES_CSV).parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ---------------- helpers ----------------\n",
    "def clean(s: str) -> str:\n",
    "    \"\"\"Normalize small text quirks for downstream parsing.\"\"\"\n",
    "    if not s:\n",
    "        return \"\"\n",
    "    s = s.replace(\"\\xa0\", \" \")  # collapse non-breaking space\n",
    "    s = s.replace(\"•\", \"\\n\")    # map bullet to line break to prevent word glue\n",
    "    s = re.sub(r\"\\bTop of page\\b\", \"\", s, flags=re.I).strip()\n",
    "    return s\n",
    "\n",
    "def insert_block_newlines(root: BeautifulSoup):\n",
    "    \"\"\"\n",
    "    Ensure block elements create hard line breaks in the extracted text.\n",
    "    Converts <br> to '\\n' and inserts '\\n' before/after common block elements.\n",
    "    Inline tags are intentionally left untouched.\n",
    "    \"\"\"\n",
    "    block_tags = {\n",
    "        \"p\",\"li\",\"tr\",\"thead\",\"tbody\",\"tfoot\",\"table\",\n",
    "        \"div\",\"section\",\"article\",\"header\",\"footer\",\n",
    "        \"h1\",\"h2\",\"h3\",\"h4\",\"h5\",\"h6\",\"dt\",\"dd\"\n",
    "    }\n",
    "    # Normalize explicit <br> into newline characters.\n",
    "    for br in root.find_all(\"br\"):\n",
    "        br.replace_with(\"\\n\")\n",
    "    # Wrap block-level tags with surrounding newlines so text extraction preserves structure.\n",
    "    for el in root.find_all(block_tags):\n",
    "        el.insert_before(NavigableString(\"\\n\"))\n",
    "        el.append(NavigableString(\"\\n\"))\n",
    "\n",
    "def node_text_with_breaks(node):\n",
    "    \"\"\"Extract text while preserving structural line breaks.\"\"\"\n",
    "    insert_block_newlines(node)\n",
    "    return node.get_text(\"\\n\")\n",
    "\n",
    "def strip_footnotes_from_dom(dom: BeautifulSoup):\n",
    "    \"\"\"\n",
    "    Remove footnote markers/links and numeric-only footnote bubbles from the DOM\n",
    "    so they won't leak into the linearized text stream.\n",
    "    \"\"\"\n",
    "    selectors = [\n",
    "        \"sup\",\n",
    "        'a[role=\"doc-noteref\"]',\n",
    "        'a[href*=\"#fn\"]', 'a[href*=\"#footnote\"]',\n",
    "        \".footnote\", \".footnotes\", \".noteref\", \".note-ref\", \".xref\"\n",
    "    ]\n",
    "    for sel in selectors:\n",
    "        for el in list(dom.select(sel)):\n",
    "            if isinstance(el, Tag):\n",
    "                el.decompose()\n",
    "    # Additionally remove naked numeric footnote artifacts like <a>12</a>.\n",
    "    for el in list(dom.find_all([\"a\", \"span\", \"sup\"])):\n",
    "        if not isinstance(el, Tag):\n",
    "            continue\n",
    "        txt = (el.get_text(strip=True) or \"\")\n",
    "        if re.fullmatch(r\"\\d{1,3}\", txt or \"\"):\n",
    "            el.decompose()\n",
    "\n",
    "def strip_inline_footnote_numbers(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Remove inline footnote numerals without harming HS patterns.\n",
    "    Handles superscripts (¹²³), parenthetical (12), bracketed [12], and\n",
    "    stray small integers attached to words.\n",
    "    \"\"\"\n",
    "    text = re.sub(r\"(?<=\\w)[\\u2070-\\u2079]+\", \"\", text)              # superscripts\n",
    "    text = re.sub(r\"(?<=\\w)\\s*\\((\\d{1,3})\\)\", \"\", text)              # (...) after word\n",
    "    text = re.sub(r\"(?<=\\w)\\s*\\[(\\d{1,3})\\]\", \"\", text)              # [...] after word\n",
    "    text = re.sub(r\"(?<=\\b[A-Za-z])\\s+(?<!\\.)\\b(\\d{1,3})\\b(?![\\d\\.])\", \" \", text)  # loose small number\n",
    "    return text\n",
    "\n",
    "def normalize_hs_notation(s: str) -> str:\n",
    "    \"\"\"\n",
    "    Normalize HS notations into a consistent dotted style:\n",
    "      - 01.01 / 03.05.71 / 11.03.11-11.03.13 / 03.04-03.06\n",
    "    Covers ranges and 2/4/6 digit normalization with dots.\n",
    "    \"\"\"\n",
    "    # Normalize full 6-digit ranges written without dots in the chapter/heading parts.\n",
    "    s = re.sub(\n",
    "        r\"\\b(\\d{4})\\.(\\d{2})\\s*-\\s*(\\d{4})\\.(\\d{2})\\b\",\n",
    "        lambda m: f\"{m.group(1)[:2]}.{m.group(1)[2:]}.{m.group(2)}-{m.group(3)[:2]}.{m.group(3)[2:]}.{m.group(4)}\",\n",
    "        s\n",
    "    )\n",
    "    # Zero-padded 6-digit ranges like 0d dd.dd - 0d dd.dd\n",
    "    s = re.sub(r\"\\b(0\\d)(\\d{2})\\.(\\d{2})\\s*-\\s*(0\\d)(\\d{2})\\.(\\d{2})\\b\", r\"\\1.\\2.\\3-\\4.\\5.\\6\", s)\n",
    "    # Zero-padded 4-digit ranges like 0d dd - 0d dd\n",
    "    s = re.sub(r\"\\b(0\\d)(\\d{2})\\s*-\\s*(0\\d)(\\d{2})\\b\", r\"\\1.\\2-\\3.\\4\", s)\n",
    "    # 6-digit single code written as 4 digits + dot + 2 digits → add a dot after the first 2 digits\n",
    "    s = re.sub(r\"\\b(\\d{4})\\.(\\d{2})\\b\", lambda m: f\"{m.group(1)[:2]}.{m.group(1)[2:]}.{m.group(2)}\", s)\n",
    "    # Zero-padded 6-digit single code already split → keep as is (idempotent)\n",
    "    s = re.sub(r\"\\b(0\\d)(\\d{2})\\.(\\d{2})\\b\", r\"\\1.\\2.\\3\", s)\n",
    "    # 4-digit heading → add dot after the first 2 digits\n",
    "    s = re.sub(r\"\\b(0\\d)(\\d{2})\\b\", r\"\\1.\\2\", s)\n",
    "    return s\n",
    "\n",
    "# ---------------- structure regex (FIXED) ----------------\n",
    "# Allow “Section I Live Animals; …” with or without a dash/colon\n",
    "RE_SECTION   = re.compile(r\"^Section\\s+([IVXLC]+)(?:\\s*[-–—:]\\s*)?(.*)$\", re.I)\n",
    "# Allow “Chapter 1: Live Animals / Chapter 1 - Live Animals / Chapter 1 Live Animals”\n",
    "RE_CHAPTER   = re.compile(r\"^(Ex\\s+)?Chapter\\s+(\\d+)\\s*[:.\\-]?\\s*(.+)$\", re.I)\n",
    "\n",
    "# 支持扩展如 1901.10.aa 或 1901.10.aa-1901.10.ab\n",
    "# 匹配标准HS到扩展item（支持 .aa, .bb, .01a 等）\n",
    "RE_RULE = re.compile(\n",
    "    r\"^(?P<hs>(\\d{2}\\.\\d{2}(?:\\.\\d{2})?(?:\\.[A-Za-z0-9]{1,4})?)(\\s*-\\s*\\d{2}\\.\\d{2}(?:\\.\\d{2})?(?:\\.[A-Za-z0-9]{1,4})?)?)\\s+(?P<rule>.+?)\\s*$\",\n",
    "    re.I,\n",
    ")\n",
    "\n",
    "RE_HS_ONLY = re.compile(\n",
    "    r\"^(?P<hs>(\\d{2}\\.\\d{2}(?:\\.\\d{2})?(?:\\.[A-Za-z0-9]{1,4})?)(\\s*-\\s*\\d{2}\\.\\d{2}(?:\\.\\d{2})?(?:\\.[A-Za-z0-9]{1,4})?)?)\\s*$\",\n",
    "    re.I,\n",
    ")\n",
    "\n",
    "\n",
    "RE_RULE_TEXT = re.compile(r\"^A\\s+change(?:\\s+to)?\\b.+$\", re.I)\n",
    "RE_NOTE      = re.compile(r\"^Note\\s*(\\d+)?\\s*:\\s*(.*)$\", re.I)\n",
    "\n",
    "# ---------------- fetch & parse ----------------\n",
    "resp = requests.get(URL, timeout=60)\n",
    "resp.raise_for_status()\n",
    "soup = BeautifulSoup(resp.text, \"lxml\")\n",
    "main = soup.select_one(\"main\") or soup\n",
    "\n",
    "# 1) Drop footnotes in DOM to avoid text noise before linearization\n",
    "strip_footnotes_from_dom(main)\n",
    "\n",
    "# 2) Extract text with structural line breaks and compress multiple blank lines\n",
    "text_all = node_text_with_breaks(main)\n",
    "text_all = re.sub(r\"\\n{2,}\", \"\\n\", text_all)\n",
    "\n",
    "# Clean per-line and remove empties\n",
    "lines = [x for x in (clean(strip_inline_footnote_numbers(t)) for t in text_all.split(\"\\n\"))]\n",
    "lines = [x for x in lines if x.strip()]\n",
    "\n",
    "# ---------------- parse state ----------------\n",
    "rules_rows = []\n",
    "notes_rows = []\n",
    "\n",
    "current_section_title = \"\"\n",
    "current_ch_label = \"\"\n",
    "current_ch_no = \"\"   # e.g., \"61\"\n",
    "current_ch_name = \"\"\n",
    "\n",
    "# Chapter-level Note accumulation (numbered notes under a chapter header)\n",
    "chapter_note_buf = []\n",
    "waiting_chapter_note = False\n",
    "chapter_note_num = None  # e.g., \"1\",\"2\"\n",
    "\n",
    "# Item-level Note accumulation (notes attached to a specific HS line)\n",
    "item_note_buf = []\n",
    "waiting_item_note = False\n",
    "item_note_hs = \"\"\n",
    "\n",
    "# Markers for the last rule/hs we appended, to support continuation lines\n",
    "last_rule_idx = None\n",
    "last_rule_hs = None\n",
    "last_hs_seen = None\n",
    "\n",
    "# Tracks if we just saw a standalone 'or.' line needing to be joined\n",
    "pending_or_continuation = False\n",
    "\n",
    "# Deduplicate chapter header insertions (some pages repeat headers)\n",
    "seen_chapter_headers = set()\n",
    "\n",
    "def flush_chapter_note():\n",
    "    \"\"\"Commit a buffered chapter note to notes_rows if present.\"\"\"\n",
    "    if current_ch_label and chapter_note_buf:\n",
    "        notes_rows.append({\n",
    "            \"scope_type\": \"chapter\",\n",
    "            \"scope_id\": current_ch_label,        # e.g., \"Chapter 61\"\n",
    "            \"chapter_name\": current_ch_name,\n",
    "            \"note_number\": chapter_note_num or \"\",\n",
    "            \"note_text\": \" \".join(chapter_note_buf).strip()\n",
    "        })\n",
    "    chapter_note_buf.clear()\n",
    "\n",
    "def flush_item_note():\n",
    "    \"\"\"Commit a buffered item (HS-level) note to notes_rows if present.\"\"\"\n",
    "    if item_note_buf and item_note_hs:\n",
    "        notes_rows.append({\n",
    "            \"scope_type\": \"item\",\n",
    "            \"scope_id\": item_note_hs,           # e.g., \"62.05.20-62.05.30\"\n",
    "            \"chapter_name\": current_ch_name,\n",
    "            \"note_number\": \"\",                  # item notes are usually unnumbered\n",
    "            \"note_text\": \" \".join(item_note_buf).strip()\n",
    "        })\n",
    "    item_note_buf.clear()\n",
    "\n",
    "for raw in lines:\n",
    "    line = normalize_hs_notation(raw)\n",
    "    line_stripped = line.strip()\n",
    "    low = line_stripped.lower()\n",
    "\n",
    "    # Early guard: avoid accidentally appending 'chapter/section' lines to prior rule\n",
    "    if low.startswith(\"chapter\") or low.startswith(\"section\"):\n",
    "        pass  # let structure matching below handle it\n",
    "\n",
    "    # ---- Section header ----\n",
    "    m = RE_SECTION.match(line)\n",
    "    if m:\n",
    "        flush_item_note()\n",
    "        flush_chapter_note()\n",
    "        current_section_title = f\"Section {m.group(1)} - {m.group(2).strip()}\" if m.group(2) else f\"Section {m.group(1)}\"\n",
    "        last_rule_idx = None\n",
    "        pending_or_continuation = False\n",
    "        continue\n",
    "\n",
    "    # ---- Chapter header ----\n",
    "    m = RE_CHAPTER.match(line)\n",
    "    if m:\n",
    "        flush_item_note()\n",
    "        flush_chapter_note()\n",
    "        chapter_note_num = None\n",
    "        current_ch_no = m.group(2).strip().zfill(2)\n",
    "        current_ch_name = m.group(3).strip()\n",
    "        current_ch_label = f\"{(m.group(1) or '').strip()}Chapter {current_ch_no}\".strip()\n",
    "        header_key = (\n",
    "            re.sub(r\"\\s+\", \" \", current_ch_label.lower()),\n",
    "            re.sub(r\"\\s+\", \" \", current_ch_name.lower())\n",
    "        )\n",
    "        # Insert one structural row per chapter to retain chapter boundaries.\n",
    "        if header_key not in seen_chapter_headers:\n",
    "            rules_rows.append({\n",
    "                \"Chapter/heading/ sub-heading\": current_ch_label,\n",
    "                \"Description\": current_ch_name,\n",
    "                \"Product Specific Rule of Origin\": \"\",\n",
    "                \"needs_manual_check\": 0\n",
    "            })\n",
    "            seen_chapter_headers.add(header_key)\n",
    "        last_rule_idx = None\n",
    "        last_rule_hs = None\n",
    "        last_hs_seen = None\n",
    "        pending_or_continuation = False\n",
    "        continue\n",
    "\n",
    "    # ---- Chapter Note start (numbered Notes right after a chapter header) ----\n",
    "    m = RE_NOTE.match(line)\n",
    "    if m and current_ch_label and not last_hs_seen:\n",
    "        waiting_chapter_note = True\n",
    "        chapter_note_num = m.group(1)\n",
    "        chapter_note_buf.append(m.group(2).strip())\n",
    "        continue\n",
    "\n",
    "    # ---- Chapter Note accumulation mode ----\n",
    "    if waiting_chapter_note:\n",
    "        # A new structural header or another Note ends the current note.\n",
    "        if RE_SECTION.match(line) or RE_CHAPTER.match(line) or RE_NOTE.match(line):\n",
    "            flush_chapter_note()\n",
    "            waiting_chapter_note = False\n",
    "            # fall through to handle this line again\n",
    "        else:\n",
    "            # If the probe is an HS within the current chapter, we close the note.\n",
    "            m_probe = RE_RULE.match(line) or RE_HS_ONLY.match(line)\n",
    "            if m_probe:\n",
    "                hs_probe = m_probe.group(\"hs\")\n",
    "                if hs_probe[:2] == current_ch_no:\n",
    "                    flush_chapter_note()\n",
    "                    waiting_chapter_note = False\n",
    "                else:\n",
    "                    chapter_note_buf.append(line.strip())\n",
    "                    continue\n",
    "            else:\n",
    "                chapter_note_buf.append(line.strip())\n",
    "                continue\n",
    "\n",
    "    # ---- HS-only line (rule text comes on a subsequent line) ----\n",
    "    m = RE_HS_ONLY.match(line)\n",
    "    if m:\n",
    "        flush_item_note()\n",
    "        last_hs_seen = m.group(\"hs\").replace(\" \", \"\")\n",
    "        last_rule_hs = None\n",
    "        last_rule_idx = None\n",
    "        continue\n",
    "\n",
    "    # ---- Item-level Note start (a Note immediately following an HS-only line) ----\n",
    "    m_item = RE_NOTE.match(line)\n",
    "    if m_item and last_hs_seen:\n",
    "        waiting_item_note = True\n",
    "        item_note_hs = last_hs_seen\n",
    "        item_note_buf.append(m_item.group(2).strip())\n",
    "        continue\n",
    "\n",
    "    # ---- Item-level Note accumulation mode ----\n",
    "    if waiting_item_note:\n",
    "        # A new structural header or Note terminates the current item note.\n",
    "        if RE_SECTION.match(line) or RE_CHAPTER.match(line) or RE_NOTE.match(line):\n",
    "            flush_item_note()\n",
    "            waiting_item_note = False\n",
    "        else:\n",
    "            # If the next line is the actual rule text for this HS, commit it and mark as manual check.\n",
    "            if RE_RULE_TEXT.match(line) and item_note_hs:\n",
    "                flush_item_note()\n",
    "                waiting_item_note = False\n",
    "                rule_text = line.strip().rstrip(\".\")\n",
    "                rules_rows.append({\n",
    "                    \"Chapter/heading/ sub-heading\": item_note_hs,\n",
    "                    \"Description\": current_ch_name,\n",
    "                    \"Product Specific Rule of Origin\": rule_text,\n",
    "                    \"needs_manual_check\": 1\n",
    "                })\n",
    "                last_rule_idx = len(rules_rows) - 1\n",
    "                last_rule_hs = item_note_hs\n",
    "                continue\n",
    "            # If we are moving on to a new HS or a combined HS+rule line, close the item note.\n",
    "            m_next_hs = RE_HS_ONLY.match(line) or RE_RULE.match(line)\n",
    "            if m_next_hs:\n",
    "                flush_item_note()\n",
    "                waiting_item_note = False\n",
    "            else:\n",
    "                # Otherwise keep accumulating the item note.\n",
    "                item_note_buf.append(line.strip())\n",
    "                continue\n",
    "\n",
    "    # ---- Standalone \"or.\" line: append to the last rule's text ----\n",
    "    if re.fullmatch(r\"or\\.?\", line, flags=re.I) and last_rule_idx is not None:\n",
    "        rules_rows[last_rule_idx][\"Product Specific Rule of Origin\"] += \" or\"\n",
    "        pending_or_continuation = True\n",
    "        continue\n",
    "\n",
    "    # ---- Continuation right after an \"or.\" line ----\n",
    "    if pending_or_continuation:\n",
    "        # Only treat as continuation if the line is not a new structural/HS/rule start.\n",
    "        if not (RE_RULE.match(line) or RE_HS_ONLY.match(line) or RE_SECTION.match(line) or RE_CHAPTER.match(line) or RE_NOTE.match(line)):\n",
    "            rules_rows[last_rule_idx][\"Product Specific Rule of Origin\"] += \" \" + line.strip()\n",
    "            pending_or_continuation = False\n",
    "            continue\n",
    "        else:\n",
    "            pending_or_continuation = False\n",
    "\n",
    "    # ---- If we just appended a rule, absorb plain text until the next structure token ----\n",
    "    if last_rule_idx is not None:\n",
    "        if not (RE_RULE.match(line) or RE_HS_ONLY.match(line) or RE_SECTION.match(line) or RE_CHAPTER.match(line) or RE_NOTE.match(line)):\n",
    "            rules_rows[last_rule_idx][\"Product Specific Rule of Origin\"] += \" \" + line.strip()\n",
    "            continue\n",
    "\n",
    "    # ---- Rule line: HS and rule present on the same line ----\n",
    "    m = RE_RULE.match(line)\n",
    "    if m:\n",
    "        flush_item_note()\n",
    "        hs = m.group(\"hs\").replace(\" \", \"\")\n",
    "        rule_text = m.group(\"rule\").strip().rstrip(\".\")\n",
    "        rules_rows.append({\n",
    "            \"Chapter/heading/ sub-heading\": hs,\n",
    "            \"Description\": current_ch_name,\n",
    "            \"Product Specific Rule of Origin\": rule_text,\n",
    "            \"needs_manual_check\": 0\n",
    "        })\n",
    "        last_rule_idx = len(rules_rows) - 1\n",
    "        last_rule_hs = hs\n",
    "        last_hs_seen = hs\n",
    "        continue\n",
    "\n",
    "    # ---- Rule text line with no HS (typically follows an HS-only line) ----\n",
    "    if RE_RULE_TEXT.match(line) and last_hs_seen:\n",
    "        flush_item_note()\n",
    "        rule_text = line.strip().rstrip(\".\")\n",
    "        rules_rows.append({\n",
    "            \"Chapter/heading/ sub-heading\": last_hs_seen,\n",
    "            \"Description\": current_ch_name,\n",
    "            \"Product Specific Rule of Origin\": rule_text,\n",
    "            \"needs_manual_check\": 0\n",
    "        })\n",
    "        last_rule_idx = len(rules_rows) - 1\n",
    "        last_rule_hs = last_hs_seen\n",
    "        continue\n",
    "\n",
    "# Finalize any buffered notes at EOF\n",
    "flush_item_note()\n",
    "flush_chapter_note()\n",
    "\n",
    "# ---- Build DataFrames and persist CSVs ----\n",
    "for r in rules_rows:\n",
    "    r.setdefault(\"needs_manual_check\", 0)\n",
    "\n",
    "rules_df = pd.DataFrame(rules_rows, columns=[\n",
    "    \"Chapter/heading/ sub-heading\",\n",
    "    \"Description\",\n",
    "    \"Product Specific Rule of Origin\",\n",
    "    \"needs_manual_check\",\n",
    "])\n",
    "notes_df = pd.DataFrame(notes_rows, columns=[\n",
    "    \"scope_type\",       # 'chapter' or 'item'\n",
    "    \"scope_id\",         # 'Chapter 61' or '62.05.20-62.05.30'\n",
    "    \"chapter_name\",     # chapter title where the note resides\n",
    "    \"note_number\",      # numbered only for chapter-level notes; usually empty for item notes\n",
    "    \"note_text\"\n",
    "])\n",
    "\n",
    "rules_df.to_csv(OUT_RULES_CSV, index=False)\n",
    "notes_df.to_csv(OUT_NOTES_CSV, index=False)\n",
    "print(f\"Saved rules: {OUT_RULES_CSV}  ({len(rules_df)} rows)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved notes: /Users/tianqixu/Downloads/PTA-task/Canada-Chile/notes.csv  (45 rows)\n"
     ]
    }
   ],
   "source": [
    "#extract the notes in a seperate file\n",
    "# -*- coding: utf-8 -*-\n",
    "# Extract ALL notes from Canada–Costa Rica Annex 301 with correct scope binding\n",
    "# pip install requests beautifulsoup4 lxml pandas\n",
    "\n",
    "import re\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup, Tag\n",
    "from pathlib import Path\n",
    "\n",
    "URL = \"https://www.international.gc.ca/trade-commerce/trade-agreements-accords-commerciaux/agr-acc/chile-chili/fta-ale/annex-d-annexe.aspx?lang=eng\"\n",
    "OUT_NOTES_CSV = \"/Users/tianqixu/Downloads/PTA-task/Canada-Chile/notes.csv\"\n",
    "Path(OUT_NOTES_CSV).parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ---------- Regex ----------\n",
    "RE_SECTION   = re.compile(r\"^\\s*Section\\s+([IVXLC]+)\\b(?:\\s*[-–—:]\\s*)?(.*)$\", re.I)\n",
    "RE_CHAPTER   = re.compile(r\"^\\s*(?:Ex\\s+)?Chapter\\s+(\\d+)\\s*[:.\\-]?\\s*(.+)$\", re.I)\n",
    "RE_NOTE      = re.compile(r\"^\\s*Note\\s*(\\d+)?\\s*:\\s*(.*)$\", re.I)\n",
    "\n",
    "# HS header patterns (support 01.01 / 01.01.10 / 01.01-01.06 / 01.01.10-01.01.90)\n",
    "RE_HS_HEAD   = re.compile(r\"^\\s*(\\d{2}\\.\\d{2}(?:\\.\\d{2})?)(?:\\s*-\\s*(\\d{2}\\.\\d{2}(?:\\.\\d{2})?))?\\b\")\n",
    "# Rule-text start (used to stop absorbing lines into a Note block)\n",
    "RE_RULE_TEXT = re.compile(r\"^\\s*A\\s+change(?:\\s+to)?\\b\", re.I)\n",
    "\n",
    "# Tags considered as block-level when linearizing content\n",
    "BLOCK_TAGS = (\"h1\",\"h2\",\"h3\",\"h4\",\"h5\",\"p\",\"li\",\"div\",\"dt\",\"dd\")\n",
    "\n",
    "# ---------- Helpers ----------\n",
    "def strip_footnotes_in_dom(root: Tag):\n",
    "    \"\"\"\n",
    "    Remove footnote superscripts/links and numeric-only artifacts directly in the DOM\n",
    "    so they do not pollute the text stream after linearization.\n",
    "    \"\"\"\n",
    "    selectors = [\n",
    "        \"sup\",\n",
    "        'a[role=\"doc-noteref\"]',\n",
    "        'a[href*=\"#fn\"]', 'a[href*=\"#footnote\"]',\n",
    "        \".footnote\", \".footnotes\", \".noteref\", \".note-ref\", \".xref\"\n",
    "    ]\n",
    "    for sel in selectors:\n",
    "        for el in list(root.select(sel)):\n",
    "            el.decompose()\n",
    "    # Also drop naked numeric bubbles like <a>12</a> or <span>3</span>.\n",
    "    for el in list(root.find_all([\"a\",\"span\",\"sup\"])):\n",
    "        t = (el.get_text(strip=True) or \"\")\n",
    "        if re.fullmatch(r\"\\d{1,3}\", t):\n",
    "            el.decompose()\n",
    "\n",
    "def block_text(el: Tag) -> str:\n",
    "    \"\"\"\n",
    "    Convert a block element into normalized text:\n",
    "      - <br> -> newline\n",
    "      - NBSP -> space\n",
    "      - bullets -> newline\n",
    "      - collapse repeated spaces/newlines\n",
    "      - remove 'Top of page' UI crumbs\n",
    "    \"\"\"\n",
    "    for br in el.find_all(\"br\"):\n",
    "        br.replace_with(\"\\n\")\n",
    "    txt = el.get_text(\"\\n\", strip=True)\n",
    "    txt = txt.replace(\"\\xa0\", \" \").replace(\"•\", \"\\n\")\n",
    "    txt = re.sub(r\"\\bTop of page\\b\", \"\", txt, flags=re.I)\n",
    "    txt = re.sub(r\"[ \\t]+\", \" \", txt)\n",
    "    txt = re.sub(r\"\\n{2,}\", \"\\n\", txt)\n",
    "    return txt.strip()\n",
    "\n",
    "# ---------- Fetch ----------\n",
    "html = requests.get(URL, timeout=60).text\n",
    "soup = BeautifulSoup(html, \"lxml\")\n",
    "main = soup.select_one(\"main\") or soup\n",
    "strip_footnotes_in_dom(main)\n",
    "\n",
    "# ---------- Linearize content as blocks ----------\n",
    "# We extract block-level text segments to preserve structural boundaries.\n",
    "blocks = []\n",
    "for el in main.find_all(BLOCK_TAGS):\n",
    "    t = block_text(el)\n",
    "    if t:\n",
    "        blocks.append(t)\n",
    "\n",
    "# ---------- Traverse & bind notes ----------\n",
    "notes = []\n",
    "\n",
    "# Current context (used to bind a Note to Section/Chapter/Item scope)\n",
    "current_section_label = \"\"\n",
    "current_section_title = \"\"\n",
    "seen_something_in_section = False\n",
    "\n",
    "current_chapter_label = \"\"\n",
    "current_chapter_name  = \"\"\n",
    "seen_hs_in_chapter    = False\n",
    "\n",
    "last_hs_range = \"\"\n",
    "last_token_type = \"\"  # 'section' | 'chapter' | 'hs' | 'rule' | 'note' | 'other'\n",
    "\n",
    "def normalize_hs_range(start_hs: str, end_hs: str | None) -> str:\n",
    "    \"\"\"Return a consistent HS range string, e.g., '05.01-05.11' or '05.01'.\"\"\"\n",
    "    return f\"{start_hs}-{end_hs}\" if end_hs else start_hs\n",
    "\n",
    "def emit_note(scope_type, scope_id, scope_title, note_no, note_text, hs_range=\"\"):\n",
    "    \"\"\"\n",
    "    Append a single normalized note record with full surrounding context\n",
    "    so downstream scripts can filter by scope or location easily.\n",
    "    \"\"\"\n",
    "    notes.append({\n",
    "        \"scope_type\": scope_type,                  # section | chapter | item\n",
    "        \"scope_id\": scope_id,                      # e.g., Section II / Chapter 05 / 05.01-05.11\n",
    "        \"scope_title\": scope_title,                # Section/Chapter human title (if present)\n",
    "        \"section_label\": current_section_label,    # Current Section container\n",
    "        \"chapter_label\": current_chapter_label,    # Current Chapter container\n",
    "        \"chapter_name\": current_chapter_name,      # Human-readable chapter name\n",
    "        \"hs_range\": hs_range,                      # HS range when scope_type=='item'\n",
    "        \"note_number\": (note_no or \"\").strip(),    # Empty for unnumbered notes\n",
    "        \"note_text\": note_text.strip()\n",
    "    })\n",
    "\n",
    "current_chapter_label = \"\"\n",
    "\n",
    "i = 0\n",
    "while i < len(blocks):\n",
    "    line = blocks[i]\n",
    "\n",
    "    # Section header\n",
    "    m = RE_SECTION.match(line)\n",
    "    if m:\n",
    "        current_section_label = f\"Section {m.group(1)}\"\n",
    "        current_section_title = (m.group(2) or \"\").strip()\n",
    "        seen_something_in_section = False\n",
    "\n",
    "        # Reset Chapter/HS context when entering a new Section\n",
    "        current_chapter_label = \"\"\n",
    "        current_chapter_name  = \"\"\n",
    "        seen_hs_in_chapter    = False\n",
    "        last_hs_range         = \"\"\n",
    "        last_token_type       = \"section\"\n",
    "        i += 1\n",
    "        continue\n",
    "\n",
    "    # Chapter header\n",
    "    m = RE_CHAPTER.match(line)\n",
    "    if m:\n",
    "        ch_no  = m.group(1).zfill(2)\n",
    "        current_chapter_label = f\"Chapter {ch_no}\"\n",
    "        current_chapter_name  = m.group(2).strip()\n",
    "        seen_something_in_section = True\n",
    "        seen_hs_in_chapter = False\n",
    "        last_hs_range = \"\"\n",
    "        last_token_type = \"chapter\"\n",
    "        i += 1\n",
    "        continue\n",
    "\n",
    "    # HS header (records the most recent HS range context)\n",
    "    m = RE_HS_HEAD.match(line)\n",
    "    if m:\n",
    "        start_hs = m.group(1)\n",
    "        end_hs   = m.group(2)\n",
    "        last_hs_range = normalize_hs_range(start_hs, end_hs)\n",
    "        seen_something_in_section = True\n",
    "        seen_hs_in_chapter = True\n",
    "        last_token_type = \"hs\"\n",
    "        i += 1\n",
    "        continue\n",
    "\n",
    "    # Rule line (used only as a boundary so Notes don't consume rule paragraphs)\n",
    "    if RE_RULE_TEXT.match(line):\n",
    "        seen_something_in_section = True\n",
    "        last_token_type = \"rule\"\n",
    "        i += 1\n",
    "        continue\n",
    "\n",
    "    # Note block (may span multiple following lines until next structural token)\n",
    "    m = RE_NOTE.match(line)\n",
    "    if m:\n",
    "        note_no = m.group(1) or \"\"\n",
    "        first = (m.group(2) or \"\").strip()\n",
    "        buf = [first] if first else []\n",
    "\n",
    "        # Consume subsequent lines into this note until a new structure boundary is encountered.\n",
    "        j = i + 1\n",
    "        while j < len(blocks):\n",
    "            probe = blocks[j]\n",
    "            if (RE_NOTE.match(probe) or RE_SECTION.match(probe) or RE_CHAPTER.match(probe) \n",
    "                or RE_HS_HEAD.match(probe) or RE_RULE_TEXT.match(probe)):\n",
    "                break\n",
    "            buf.append(probe.strip())\n",
    "            j += 1\n",
    "\n",
    "        note_text = \" \".join(buf).strip()\n",
    "\n",
    "        # Scope binding priority:\n",
    "        #   1) Section-level: immediately after a Section, before any Chapter/HS\n",
    "        #   2) Chapter-level: first notes after a Chapter, before any HS\n",
    "        #   3) Item-level: adjacent to the most recent HS\n",
    "        #   4) Fallback: Chapter if available, else Section\n",
    "        if last_token_type == \"section\" and not current_chapter_label and not last_hs_range:\n",
    "            emit_note(\n",
    "                scope_type=\"section\",\n",
    "                scope_id=current_section_label,\n",
    "                scope_title=current_section_title,\n",
    "                note_no=note_no,\n",
    "                note_text=note_text\n",
    "            )\n",
    "\n",
    "        elif last_token_type == \"chapter\" and not seen_hs_in_chapter:\n",
    "            emit_note(\n",
    "                scope_type=\"chapter\",\n",
    "                scope_id=current_chapter_label,\n",
    "                scope_title=current_chapter_name,\n",
    "                note_no=note_no,\n",
    "                note_text=note_text\n",
    "            )\n",
    "\n",
    "        elif last_hs_range and last_token_type in (\"hs\", \"note\"):\n",
    "            # Treat as item-level when the closest anchor is an HS (or we just handled a note near HS)\n",
    "            emit_note(\n",
    "                scope_type=\"item\",\n",
    "                scope_id=last_hs_range,\n",
    "                scope_title=current_chapter_name,\n",
    "                note_no=note_no,\n",
    "                note_text=note_text,\n",
    "                hs_range=last_hs_range\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            # Conservative fallback: prefer Chapter if available, else Section.\n",
    "            if current_chapter_label:\n",
    "                emit_note(\n",
    "                    scope_type=\"chapter\",\n",
    "                    scope_id=current_chapter_label,\n",
    "                    scope_title=current_chapter_name,\n",
    "                    note_no=note_no,\n",
    "                    note_text=note_text\n",
    "                )\n",
    "            else:\n",
    "                emit_note(\n",
    "                    scope_type=\"section\",\n",
    "                    scope_id=current_section_label,\n",
    "                    scope_title=current_section_title,\n",
    "                    note_no=note_no,\n",
    "                    note_text=note_text\n",
    "                )\n",
    "\n",
    "        last_token_type = \"note\"\n",
    "        i = j\n",
    "        continue\n",
    "\n",
    "    # Plain text (ignored for note-extraction; used only to mark activity)\n",
    "    seen_something_in_section = True\n",
    "    last_token_type = \"other\"\n",
    "    i += 1\n",
    "\n",
    "# ---------- Save ----------\n",
    "df = pd.DataFrame(notes, columns=[\n",
    "    \"scope_type\",\"scope_id\",\"scope_title\",\n",
    "    \"section_label\",\"chapter_label\",\"chapter_name\",\"hs_range\",\n",
    "    \"note_number\",\"note_text\"\n",
    "])\n",
    "df.to_csv(OUT_NOTES_CSV, index=False)\n",
    "print(f\"✅ Saved notes: {OUT_NOTES_CSV}  ({len(df)} rows)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================\n",
    "# 1️⃣ Improved rule-splitting utilities (avoid splitting list items)\n",
    "# ==============================================================\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "SPLIT_PATTERN = re.compile(\n",
    "    r\"\"\"\n",
    "    ;\\s*or\\s*(?=\\n)                                              # line ends with '; or' + newline\n",
    "    |;\\s*or\\s+(?=(A\\s+change|No\\s+required\\s+change|Provided\\b|A\\s+regional\\s+value\\s+content\\b))  # '; or' + next full rule\n",
    "    |\\bor\\s*\\n+(?=(A\\s+change|No\\s+required\\s+change|Provided\\b|A\\s+regional\\s+value\\s+content\\b))           # standalone 'or' + next full rule\n",
    "    \"\"\",\n",
    "    re.IGNORECASE | re.VERBOSE\n",
    ")\n",
    "\n",
    "def tidy(s: str) -> str:\n",
    "    \"\"\"Normalize whitespace and newlines for stable pattern matching.\"\"\"\n",
    "    if not isinstance(s, str):\n",
    "        return \"\"\n",
    "    s = s.replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\")\n",
    "    s = re.sub(r\"[ \\t]+\", \" \", s)\n",
    "    s = re.sub(r\"\\n{2,}\", \"\\n\", s)\n",
    "    return s.strip(\" \\n;\")\n",
    "\n",
    "def split_rule_text(text: str, max_splits: int = 3):\n",
    "    \"\"\"\n",
    "    Split one rule string into up to 3 logical parts:\n",
    "      main_rule, alt_rule, alt_rule_2\n",
    "\n",
    "    Logic:\n",
    "    - Split only when '; or' or standalone 'or' clearly introduce a new full sentence rule\n",
    "      starting with 'A change', 'No required change', 'Provided', or similar.\n",
    "    - Do NOT split when '; or' is followed by '(a)', '(b)', '(c)', etc.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return [\"\", \"\", \"\"]\n",
    "    t = tidy(text)\n",
    "\n",
    "    # sequentially find split points\n",
    "    parts = []\n",
    "    last_end = 0\n",
    "    for m in SPLIT_PATTERN.finditer(t):\n",
    "        # Look ahead context: if '; or (a)' etc., skip splitting\n",
    "        after = t[m.end():m.end()+5].strip()\n",
    "        if re.match(r\"^\\([a-z]\\)\", after, flags=re.I):  # skip list items\n",
    "            continue\n",
    "        head = tidy(t[last_end:m.start()])\n",
    "        if head:\n",
    "            parts.append(head)\n",
    "        last_end = m.end()\n",
    "        if len(parts) >= max_splits - 1:\n",
    "            break\n",
    "    tail = tidy(t[last_end:])\n",
    "    if tail:\n",
    "        parts.append(tail)\n",
    "    while len(parts) < 3:\n",
    "        parts.append(\"\")\n",
    "    return parts[:3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: /Users/tianqixu/Downloads/PTA-task/Canada-Chile/roo（1）.csv  (rows=1182)\n",
      "has_alt_rule\n",
      "0    871\n",
      "1    311\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================\n",
    "# 2️⃣  Apply rule splitting to the existing CSV\n",
    "# ==============================================================\n",
    "\n",
    "IN_CSV  = \"/Users/tianqixu/Downloads/PTA-task/Canada-Chile/roo.csv\"   # original scraped file\n",
    "OUT_CSV = \"/Users/tianqixu/Downloads/PTA-task/Canada-Chile/roo（1）.csv\" # intermediate output\n",
    "\n",
    "MAIN_COL = \"Product Specific Rule of Origin\"\n",
    "\n",
    "# --- Load dataset ---\n",
    "df = pd.read_csv(IN_CSV)\n",
    "\n",
    "# --- Basic validation ---\n",
    "if MAIN_COL not in df.columns:\n",
    "    raise ValueError(f\"Column not found: {MAIN_COL}\\nAvailable columns: {list(df.columns)}\")\n",
    "\n",
    "# --- Perform multi-rule splitting ---\n",
    "rules = df[MAIN_COL].fillna(\"\").astype(str).tolist()\n",
    "split_main, split_alt, split_alt2 = [], [], []\n",
    "\n",
    "for txt in rules:\n",
    "    r1, r2, r3 = split_rule_text(txt)\n",
    "    split_main.append(r1)\n",
    "    split_alt.append(r2)\n",
    "    split_alt2.append(r3)\n",
    "\n",
    "# --- Write new columns ---\n",
    "df[\"main_rule\"]   = split_main\n",
    "df[\"alt_rule\"]    = split_alt\n",
    "df[\"alt_rule_2\"]  = split_alt2\n",
    "\n",
    "# Binary flag: 1 if at least one alternative rule exists\n",
    "df[\"has_alt_rule\"] = (\n",
    "    df[\"alt_rule\"].str.len().gt(0) | df[\"alt_rule_2\"].str.len().gt(0)\n",
    ").astype(int)\n",
    "\n",
    "# --- Save output ---\n",
    "df.to_csv(OUT_CSV, index=False)\n",
    "print(f\"Saved: {OUT_CSV}  (rows={len(df)})\")\n",
    "\n",
    "# --- Quick distribution check ---\n",
    "print(df[\"has_alt_rule\"].value_counts(dropna=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cleaned file saved to: /Users/tianqixu/Downloads/PTA-task/Canada-Chile/roo（1）.csv  (1182 rows)\n",
      "Final columns: ['Chapter/heading/ sub-heading', 'Description', 'main_rule', 'alt_rule', 'alt_rule_2']\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================\n",
    "# 3️⃣  Final cleanup and column renaming\n",
    "# ==============================================================\n",
    "\n",
    "OUT_CLEAN = \"/Users/tianqixu/Downloads/PTA-task/Canada-Chile/roo（1）.csv\"\n",
    "\n",
    "# Columns that are redundant for final release\n",
    "cols_to_drop = [\"Product Specific Rule of Origin\", \"needs_manual_check\", \"has_alt_rule\"]\n",
    "df = df.drop(columns=[c for c in cols_to_drop if c in df.columns], errors=\"ignore\")\n",
    "\n",
    "# Ensure consistent column naming\n",
    "df = df.rename(columns={\"main_rule\": \"main_rule\", \"alt_rule\": \"alt_rule\", \"alt_rule_2\": \"alt_rule_2\"})\n",
    "\n",
    "# Save the cleaned dataset\n",
    "df.to_csv(OUT_CLEAN, index=False)\n",
    "\n",
    "print(f\"\\nCleaned file saved to: {OUT_CLEAN}  ({len(df)} rows)\")\n",
    "print(\"Final columns:\", list(df.columns))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flag summary (1 = alt_rule_2 has content, 0 = empty):\n",
      "flag\n",
      "0    1179\n",
      "1       3\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sample rows with flag = 1 (non-empty alt_rule_2):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Chapter/heading/ sub-heading</th>\n",
       "      <th>main_rule</th>\n",
       "      <th>alt_rule</th>\n",
       "      <th>alt_rule_2</th>\n",
       "      <th>flag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>930</th>\n",
       "      <td>85.23.52</td>\n",
       "      <td>A change to any other “smart card” of subheadi...</td>\n",
       "      <td>A change to any other “smart card” of subheadi...</td>\n",
       "      <td>No required change in tariff classification to...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1014</th>\n",
       "      <td>87.08.50</td>\n",
       "      <td>A change to a drive-axle with differential, wh...</td>\n",
       "      <td>A change to any other non-driving axle and par...</td>\n",
       "      <td>No required change in tariff classification to...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1148</th>\n",
       "      <td>95.03</td>\n",
       "      <td>A change to a doll representing only a human b...</td>\n",
       "      <td>A change to a doll representing only a human b...</td>\n",
       "      <td>A change to any other good of heading 95.03 fr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Chapter/heading/ sub-heading  \\\n",
       "930                      85.23.52   \n",
       "1014                     87.08.50   \n",
       "1148                        95.03   \n",
       "\n",
       "                                              main_rule  \\\n",
       "930   A change to any other “smart card” of subheadi...   \n",
       "1014  A change to a drive-axle with differential, wh...   \n",
       "1148  A change to a doll representing only a human b...   \n",
       "\n",
       "                                               alt_rule  \\\n",
       "930   A change to any other “smart card” of subheadi...   \n",
       "1014  A change to any other non-driving axle and par...   \n",
       "1148  A change to a doll representing only a human b...   \n",
       "\n",
       "                                             alt_rule_2  flag  \n",
       "930   No required change in tariff classification to...     1  \n",
       "1014  No required change in tariff classification to...     1  \n",
       "1148  A change to any other good of heading 95.03 fr...     1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Flagged file saved to: /Users/tianqixu/Downloads/PTA-task/Canada-Chile/roo_flagged.csv\n"
     ]
    }
   ],
   "source": [
    "# === Flag rows ONLY where alt_rule_2 has content ===\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load the cleaned file\n",
    "file_path = \"/Users/tianqixu/Downloads/PTA-task/Canada-Chile/roo（1）.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Create flag: 1 if alt_rule_2 has non-empty text, else 0\n",
    "df[\"flag\"] = df[\"alt_rule_2\"].fillna(\"\").str.strip().ne(\"\").astype(int)\n",
    "\n",
    "# Summary of flag distribution\n",
    "print(\"Flag summary (1 = alt_rule_2 has content, 0 = empty):\")\n",
    "print(df[\"flag\"].value_counts(dropna=False))\n",
    "\n",
    "# Optionally display a few flagged rows\n",
    "if df[\"flag\"].sum() > 0:\n",
    "    print(\"\\nSample rows with flag = 1 (non-empty alt_rule_2):\")\n",
    "    display(\n",
    "        df.loc[df[\"flag\"] == 1, \n",
    "               [\"Chapter/heading/ sub-heading\", \"main_rule\", \"alt_rule\", \"alt_rule_2\", \"flag\"]].head(10)\n",
    "    )\n",
    "else:\n",
    "    print(\"\\nNo rows have non-empty alt_rule_2.\")\n",
    "\n",
    "# Save updated file\n",
    "out_path = \"/Users/tianqixu/Downloads/PTA-task/Canada-Chile/roo_flagged.csv\"\n",
    "df.to_csv(out_path, index=False)\n",
    "print(f\"\\nFlagged file saved to: {out_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Cleaned file saved to:\n",
      "/Users/tianqixu/Downloads/PTA-task/Canada-Chile/roo_flagged.csv (1182 rows)\n",
      "Columns now: ['Chapter/heading/ sub-heading', 'Description', 'main_rule', 'alt_rule', 'alt_rule_2', 'flag']\n"
     ]
    }
   ],
   "source": [
    "# ==== Remove redundant columns and rename ====\n",
    "\n",
    "# Columns that are no longer needed in the final output\n",
    "cols_to_drop = [\"Product Specific Rule of Origin\", \"needs_manual_check\", \"has_alt_rule\"]\n",
    "\n",
    "# Drop only the columns that actually exist (for compatibility across different versions)\n",
    "df = df.drop(columns=[c for c in cols_to_drop if c in df.columns], errors=\"ignore\")\n",
    "\n",
    "# Rename 'rule' → 'main_rule' for consistency across datasets\n",
    "df = df.rename(columns={\"rule\": \"main_rule\"})\n",
    "\n",
    "# ==== Save cleaned output ====\n",
    "OUT_CLEAN = \"/Users/tianqixu/Downloads/PTA-task/Canada-Chile/roo_flagged.csv\"\n",
    "df.to_csv(OUT_CLEAN, index=False)\n",
    "\n",
    "print(f\" Cleaned file saved to:\\n{OUT_CLEAN} ({len(df)} rows)\")\n",
    "print(\"Columns now:\", list(df.columns))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
